{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaU9_sOGzRD1"
      },
      "source": [
        "# Neural Networks\n",
        "In this notebook we will learn how to train a simple Multilayer Perceptron for image classification using PyTorch. You can find additional information [here](https://pytorch.org/tutorials/beginner/basics/intro.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o30wUG8zRD7"
      },
      "outputs": [],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvfTDUXuzRD9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDBhCwhszREA"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5IQVCmOzREB"
      },
      "outputs": [],
      "source": [
        "# torchvision has some datasets already included, so we will load MNIST through torchvision\n",
        "# first we need to define the transformations\n",
        "\n",
        "data_aug = transforms.Compose([transforms.ToTensor()]) # the ToTensor transform scales the image into [0., 1.0] range\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=data_aug,\n",
        ")\n",
        "validation_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=data_aug,\n",
        ")\n",
        "indices = list(range(len(validation_data)))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "test_size = 0.2 * len(indices)\n",
        "split = int(np.floor(test_size))\n",
        "val_idx, test_idx = indices[split:], indices[:split]\n",
        "\n",
        "val_sampler = SubsetRandomSampler(val_idx)\n",
        "test_sampler = SubsetRandomSampler(test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByDvrH3z1uOT"
      },
      "outputs": [],
      "source": [
        "# now we need to define a Dataloader, which allows us to automatically batch our inputs, do sampling and multiprocess data loading\n",
        "batch_size = 64\n",
        "num_workers = 2 # how many processes are used to load the data\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=val_sampler, batch_size=batch_size, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "test_dataloader = DataLoader(validation_data, sampler=test_sampler, batch_size=1, shuffle=False, num_workers=num_workers, drop_last=False)\n",
        "\n",
        "# let's visualize the data\n",
        "# alternative 1: using the Dataset\n",
        "sample = training_data[0] \n",
        "img = sample[0]\n",
        "label = sample[1]\n",
        "print(img.shape) # note that here we only get one image and its label\n",
        "print(label)\n",
        "\n",
        "# alternative 2: iterate over the Dataloader\n",
        "for batch in train_dataloader:\n",
        "  imgs = batch[0]\n",
        "  labels = batch[1]\n",
        "  print(imgs.shape)\n",
        "  print(labels)\n",
        "\n",
        "  plt.imshow(imgs[0][0,:,:], cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aqWR_0VzRED"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfK3c9RSzRED"
      },
      "outputs": [],
      "source": [
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork().to(device) # put model in device (GPU or CPU)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1kRGiw_zREE"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNa9_1jhzREE"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss() # already includes the Softmax activation\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxSdayviCWk5"
      },
      "outputs": [],
      "source": [
        "def epoch_iter(dataloader, model, loss_fn, optimizer=None, is_train=True):\n",
        "    if is_train:\n",
        "      assert optimizer is not None, \"When training, please provide an optimizer.\"\n",
        "      \n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    if is_train:\n",
        "      model.train() # put model in train mode\n",
        "    else:\n",
        "      model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.set_grad_enabled(is_train):\n",
        "      for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # Compute prediction error\n",
        "          pred = model(X)\n",
        "          loss = loss_fn(pred, y)\n",
        "\n",
        "          if is_train:\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "          # Save training metrics\n",
        "          total_loss += loss.item() # IMPORTANT: call .item() to obtain the value of the loss WITHOUT the computational graph attached\n",
        "\n",
        "          probs = F.softmax(pred, dim=1)\n",
        "          final_pred = torch.argmax(probs, dim=1)\n",
        "          preds.extend(final_pred.cpu().numpy())\n",
        "          labels.extend(y.cpu().numpy())\n",
        "\n",
        "    return total_loss / num_batches, accuracy_score(labels, preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmsUVGS6C0O1"
      },
      "outputs": [],
      "source": [
        "num_epochs = 10\n",
        "train_history = {'loss': [], 'accuracy': []}\n",
        "val_history = {'loss': [], 'accuracy': []}\n",
        "best_val_loss = np.inf\n",
        "print(\"Start training...\")\n",
        "for t in range(num_epochs):\n",
        "    print(f\"\\nEpoch {t+1}\")\n",
        "    train_loss, train_acc = epoch_iter(train_dataloader, model, loss_fn, optimizer)\n",
        "    print(f\"Train loss: {train_loss:.3f} \\t Train acc: {train_acc:.3f}\")\n",
        "    val_loss, val_acc = epoch_iter(validation_dataloader, model, loss_fn, is_train=False)\n",
        "    print(f\"Val loss: {val_loss:.3f} \\t Val acc: {val_acc:.3f}\")\n",
        "\n",
        "    # save model when val loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
        "      torch.save(save_dict, 'best_model.pth')\n",
        "\n",
        "    # save latest model\n",
        "    save_dict = {'model': model.state_dict(), 'optimizer': optimizer.state_dict(), 'epoch': t}\n",
        "    torch.save(save_dict, 'latest_model.pth')\n",
        "\n",
        "    # save training history for plotting purposes\n",
        "    train_history[\"loss\"].append(train_loss)\n",
        "    train_history[\"accuracy\"].append(train_acc)\n",
        "\n",
        "    val_history[\"loss\"].append(val_loss)\n",
        "    val_history[\"accuracy\"].append(val_acc)\n",
        "    \n",
        "print(\"Finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrQMAKFHzREG"
      },
      "source": [
        "## Analyse training evolution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr48TEVlzREH"
      },
      "outputs": [],
      "source": [
        "def plotTrainingHistory(train_history, val_history):\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.title('Cross Entropy Loss')\n",
        "    plt.plot(train_history['loss'], label='train')\n",
        "    plt.plot(val_history['loss'], label='val')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.title('Classification Accuracy')\n",
        "    plt.plot(train_history['accuracy'], label='train')\n",
        "    plt.plot(val_history['accuracy'], label='val')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.legend(loc='best')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GfeNPc4zREI"
      },
      "outputs": [],
      "source": [
        "plotTrainingHistory(train_history, val_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPZLw5cfzREI"
      },
      "source": [
        "## Test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtmFHipizREK"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model in the test set\n",
        "\n",
        "# load the best model (i.e. model with the lowest val loss...might not be the last model)\n",
        "# we could also load the optimizer and resume training if needed\n",
        "\n",
        "model = NeuralNetwork().to(device)\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "\n",
        "test_loss, test_acc = epoch_iter(test_dataloader, model, loss_fn, is_train=False)\n",
        "print(f\"\\nTest Loss: {test_loss:.3f} \\nTest Accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1yyEC9pzREJ"
      },
      "outputs": [],
      "source": [
        "def showErrors(model, dataloader, num_examples=20):    \n",
        "    plt.figure(figsize=(15, 15))\n",
        "\n",
        "    for ind, (X, y) in enumerate(dataloader):\n",
        "      if ind >= 20: break\n",
        "      X, y = X.to(device), y.to(device)    \n",
        "      pred = model(X)\n",
        "      probs = F.softmax(pred, dim=1)\n",
        "      final_pred = torch.argmax(probs, dim=1)\n",
        "\n",
        "      plt.subplot(10, 10, ind + 1)\n",
        "      plt.axis(\"off\")\n",
        "      plt.text(0, -1, y[0].item(), fontsize=14, color='green') # correct\n",
        "      plt.text(8, -1, final_pred[0].item(), fontsize=14, color='red')  # predicted\n",
        "      plt.imshow(X[0][0,:,:].cpu(), cmap='gray')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh38evzTzREL"
      },
      "outputs": [],
      "source": [
        "showErrors(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Challenges\n",
        "\n",
        "a) As the test accuracy should show, the MNIST dataset is not very challenging, change the code to use Fashion-MNIST and compare the results.\n",
        "\n",
        "b) Do the same for the CIFAR10 (or CIFAR100) dataset. Note that, in this case, each image is a 32x32 color image; convert it to grayscale or concatenate the RGB channels in one single vector (e.g. using the reshape method).\n",
        "\n",
        "c) The test accuracy for CIFAR is significantly worse. Try improving the results by using: 1) a deeper architecture, and 2) a different optmizer.\n",
        "\n",
        "You can load the datasets from [here](https://pytorch.org/vision/stable/datasets.html).\n"
      ],
      "metadata": {
        "id": "Oq-C6glKseuO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nn_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}