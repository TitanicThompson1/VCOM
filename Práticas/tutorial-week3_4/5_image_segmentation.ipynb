{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "imagesDir = 'Images' # Change this, according to your images' directory path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(imagesDir, 'sudoku.png'))\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.medianBlur(img, 5)\n",
    "\n",
    "# global thresholding\n",
    "ret, th1 = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# otsu's thresholding (after gaussian filtering)\n",
    "blur = cv2.GaussianBlur(img,(5,5),0)\n",
    "ret2, th2 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "\n",
    "# adaptive thresholding\n",
    "th3 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 2) # change to cv.ADAPTIVE_THRESH_GAUSSIAN_C for adaptive gaussian thresholding\n",
    "\n",
    "titles = ['Original Image', 'Global Thresholding\\n(v = 127)', 'Otsu Thresholding',\n",
    "            'Adaptive Mean \\nThresholding']\n",
    "images = [img, th1, th2, th3]\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(images[i],'gray')\n",
    "    plt.title(titles[i])\n",
    "    plt.axis('off')\n",
    "plt.subplots_adjust(hspace = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[K-Means](https://docs.opencv.org/master/d5/d38/group__core__cluster.html#ga9a34dc06c6ec9460e90860f15bcd2f88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = cv2.imread(os.path.join(imagesDir, 'home.jpg'))\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img2)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = img2.reshape((-1,3))\n",
    "Z = np.float32(Z)\n",
    "\n",
    "# Define criteria, number of clusters(K) and apply kmeans\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "k = 4\n",
    "ret, label, center = cv2.kmeans(Z, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to uint8, and make resulting image\n",
    "center = np.uint8(center)\n",
    "result = center[label.flatten()]\n",
    "result = result.reshape((img2.shape))\n",
    "\n",
    "plt.imshow(result)\n",
    "plt.title('K-Means Segmentation')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation with Watersheds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Watershed](https://docs.opencv.org/4.5.1/d7/d1b/group__imgproc__misc.html#ga3267243e4d3f95165d55a618c65ac6e1)\n",
    "\n",
    "Please check [this page](https://docs.opencv.org/4.x/d3/db4/tutorial_py_watershed.html) to fully follow and comprehend all the morphological operations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = cv2.imread(os.path.join(imagesDir, 'water_coins.jpg')) # Change this, according to your image's path\n",
    "coins = cv2.cvtColor(coins, cv2.COLOR_BGR2RGB)\n",
    "coinsGray = cv2.cvtColor(coins, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "plt.imshow(coinsGray, cmap='gray')\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Step: finding an approximate estimate of the coins, using Otsuâ€™s binarization\n",
    "ret, thresh = cv2.threshold(coinsGray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "plt.imshow(thresh, cmap='gray')\n",
    "plt.title('Image after Otsu')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Noise\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations = 2)\n",
    "\n",
    "# Getting the background area\n",
    "bg = cv2.dilate(opening,kernel,iterations=3)\n",
    "\n",
    "# Getting the foreground area\n",
    "distTransform = cv2.distanceTransform(opening, cv2.DIST_L2,5)\n",
    "ret, fg = cv2.threshold(distTransform, 0.7 * distTransform.max(), 255, 0)\n",
    "\n",
    "# Getting the \"unknown\" area\n",
    "fg = np.uint8(fg)\n",
    "unknown = cv2.subtract(bg, fg)\n",
    "\n",
    "plt.imshow(distTransform, cmap='gray')\n",
    "plt.title('Distance Transform')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(fg, cmap='gray')\n",
    "plt.title('Foreground')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marker labelling\n",
    "ret, markers = cv2.connectedComponents(fg)\n",
    "\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers + 1\n",
    "\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown==255] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally apply Watershed\n",
    "markers = cv2.watershed(coins,markers)\n",
    "coins[markers == -1] = [255,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(markers, cmap='Spectral')\n",
    "plt.title('Marker Image After Segmentation')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(coins)\n",
    "plt.title('Watershed Segmentation Result')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation with GrabCut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GrabCut](https://docs.opencv.org/4.x/d3/d47/group__imgproc__segmentation.html#ga909c1dda50efcbeaa3ce126be862b37f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening an image\n",
    "img3 = cv2.imread(os.path.join(imagesDir, 'giraffe.jpg')) # Change this, according to your image's path\n",
    "img3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img3)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Defining the image mask for the GrabCut output; it has the same spatial dimensions as the input image\n",
    "mask = np.zeros(img3.shape[:2], np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bounding box coordinates that approximately delimitates the object of interest: (x, y, width, heigh)\n",
    "rect = (0, 0, 600, 800)\n",
    "\n",
    "# Allocating memory for the two arrays that this algorithm internally uses for the segmentation of the foreground and background\n",
    "bgModel = np.zeros((1,65), np.float64)\n",
    "fgModel = np.zeros((1,65), np.float64)\n",
    "\n",
    "# Number of iterations the algorithm will run\n",
    "nIter = 5\n",
    "\n",
    "# Applying GrabCut, using the defined bounding box\n",
    "(mask, bgModel, fgModel) = cv2.grabCut(img3, mask, rect, bgModel, fgModel, nIter, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# All definite background and probable background pixels are set to 0, and all definite foreground and probable foreground pixels are set to 1\n",
    "outputMask = np.where((mask == cv2.GC_BGD) | (mask == cv2.GC_PR_BGD), 0, 1)\n",
    "\n",
    "# Scale the mask from the range [0, 1] to [0, 255]\n",
    "outputMask = (outputMask * 255).astype(\"uint8\")\n",
    "\n",
    "# Apply a bitwise AND to the image using the generated mask by\n",
    "# GrabCut, obtaining the final output\n",
    "grabcut_output = cv2.bitwise_and(img3, img3, mask=outputMask)\n",
    "plt.imshow(grabcut_output)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
